{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM and Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Lasso and Ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between LASSO and Ridge regression is due to their different regularization model (L1 norm vs. L2 norm). The regularization term in LASSO is $\\lambda||\\mathbf w||_1$, while the regularization term in ridge regression is  $(\\lambda/2) ||\\mathbf w||^2$ (where $\\mathbf w$ denotes the set of parameters for the linear regression model and $\\lambda$ is the trade-off regularization parameter). LASSO typically enforces more _sparsity_ on the resulting $\\mathbf w$. That is, the resulting classifier will have a small number of non-zero weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-30T23:46:14.931653Z",
     "start_time": "2018-10-30T23:46:14.919868Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "class DataA:\n",
    "    def __init__(self):\n",
    "        f = lambda x, y : np.random.randn(x, y)\n",
    "        self.train_x = f(1000, 20)\n",
    "        self.train_y = f(1000, 1)[:,0]\n",
    "        self.test_x = f(500, 20)\n",
    "        self.test_y = f(500, 1)[:,0]\n",
    "        \n",
    "class DataB:\n",
    "    def __init__(self):\n",
    "        # Data from: https://archive.ics.uci.edu/ml/datasets/Cloud\n",
    "        data = np.fromfile(\"data/cloud.data\", sep = \" \").reshape((1024, 10))\n",
    "        y = data[:, 6]\n",
    "        X = np.delete(data, 6, axis = 1)\n",
    "        \n",
    "        self.train_x = X[:800]\n",
    "        self.train_y = y[:800]\n",
    "        \n",
    "        self.test_x = X[800:]\n",
    "        self.test_y = y[800:]\n",
    "        \n",
    "class DataC:\n",
    "    def __init__(self):\n",
    "        # Data from: http://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "        data = pd.read_csv(\"data/forestfires.csv\")\n",
    "        data = data.sample(frac = 1).reset_index(drop = True).drop(columns = [\"month\", \"day\"])\n",
    "        data[\"area\"] = np.log(data[\"area\"] + 1)\n",
    "        X = data.drop(columns = \"area\").values\n",
    "        y = data[\"area\"].values\n",
    "        \n",
    "        self.train_x = X[:400]\n",
    "        self.train_y = y[:400]\n",
    "        \n",
    "        self.test_x = X[400:]\n",
    "        self.test_y = y[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a = DataA()\n",
    "data_b = DataB()\n",
    "data_c = DataC()\n",
    "#print(np.shape(data_a.train_x[:,0]))\n",
    "#plt.scatter(data_a.train_x[:,0],data_a.train_y)\n",
    "#plt.title('Data A with X[0]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Train a LASSO model using 5 different values for the regularization parameter $\\lambda$.\n",
    "2. Report the value of $\\lambda$ that yields the minimum number of non-zero coefficients in the resulting $\\mathbf w$, and report the number of non-zero coefficients in that case. \n",
    "3. For each of the classifiers learned in part (A), compute their test error as mean-squared-error. Plot the test error as function of $\\lambda$.\n",
    "4. Report the value of $\\lambda$ that yields the $\\mathbf w$ with the minimum test error. Save this $\\mathbf w$ as $\\mathbf w_d$. \n",
    "\n",
    "Note: $\\lambda$ is same as $\\alpha$ in the sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "coeffs_dataA = []\n",
    "coeffs_dataB = []\n",
    "coeffs_dataC = []\n",
    "predA = []\n",
    "predB = []\n",
    "predC = []\n",
    "\n",
    "# alpha values are also called regularization Parameters\n",
    "\n",
    "regularizationParam = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "#regularizationParam['alpha']\n",
    "\n",
    "\"\"\"\n",
    " Construct Lasso models for each dataset.\n",
    " Once the model is trained on training set, use predict method to predict values of y.\n",
    " Next, calculate Mean-Squared Errors and also get the count of non-zero coeeficients.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for value in regularizationParam:\n",
    "    #print(values[i])\n",
    "    lassoRegressiondataA = Lasso(alpha = value)\n",
    "    lassoModeldataA = lassoRegressiondataA.fit(data_a.train_x, data_a.train_y)\n",
    "    lassoRegressiondataB = Lasso(alpha = value)\n",
    "    lassoModeldataB = lassoRegressiondataB.fit(data_b.train_x, data_b.train_y)\n",
    "    lassoRegressiondataC = Lasso(alpha = value)\n",
    "    lassoModeldataC = lassoRegressiondataC.fit(data_c.train_x, data_c.train_y)\n",
    "    coeffs_dataA.append(np.count_nonzero(lassoModeldataA.coef_))\n",
    "    predA.append(lassoRegressiondataA.predict(data_a.test_x))\n",
    "    coeffs_dataB.append(np.count_nonzero(lassoModeldataB.coef_))\n",
    "    predB.append(lassoRegressiondataB.predict(data_b.test_x))\n",
    "    coeffs_dataC.append(np.count_nonzero(lassoModeldataC.coef_))\n",
    "    predC.append(lassoRegressiondataC.predict(data_c.test_x))\n",
    "    \n",
    "    \"\"\"\n",
    "    The following print statements print out the number of non-zero coefficients for each value of alpha\n",
    "    for each dataset.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Number of non-zero coefficients (datset A) for ALPHA =',value,' is ',np.count_nonzero(lassoModeldataA.coef_), 'out of ',len(lassoModeldataA.coef_))\n",
    "    print('Number of non-zero coefficients (dataset B) for ALPHA =',value,' is ',np.count_nonzero(lassoModeldataB.coef_), 'out of ',len(lassoModeldataB.coef_))\n",
    "    print('Number of non-zero coefficients (dataset C) for ALPHA =',value,' is ',np.count_nonzero(lassoModeldataC.coef_), 'out of ',len(lassoModeldataC.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the figures ###\n",
    "\n",
    "* First figure plots the number of non-zero coefficients for each dataset for each value of $\\lambda$\n",
    "\n",
    "* Second figure plots the mean squared error for each value of $\\lambda$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(regularizationParam, coeffs_dataA)\n",
    "ax.scatter(regularizationParam, coeffs_dataB)\n",
    "ax.scatter(regularizationParam, coeffs_dataC)\n",
    "\n",
    "ax.plot(regularizationParam, coeffs_dataA,label = 'Dataset A')\n",
    "ax.plot(regularizationParam, coeffs_dataB,label = 'Dataset B')\n",
    "ax.plot(regularizationParam, coeffs_dataC, label = 'Dataset C')\n",
    "plt.title('Alpha vs number of non-zero coefficients for Lasso Regression Model')\n",
    "plt.xlabel('Alpha (Regularization parameter)')\n",
    "plt.ylabel('Number of non-zero coefficients')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "MSE_A = []\n",
    "MSE_B = []\n",
    "MSE_C = []\n",
    " \n",
    "for count in range(len(regularizationParam)):\n",
    "    MSE_A.append(mean_squared_error(data_a.test_y,predA[count]))\n",
    "    MSE_B.append(mean_squared_error(data_b.test_y,predB[count]))\n",
    "    MSE_C.append(mean_squared_error(data_c.test_y,predC[count]))\n",
    "    \n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(regularizationParam,MSE_A)\n",
    "ax.scatter(regularizationParam,MSE_B)\n",
    "ax.scatter(regularizationParam,MSE_C)\n",
    "ax.plot(regularizationParam,MSE_A, label = 'Dataset A')\n",
    "ax.plot(regularizationParam,MSE_B, label = 'Dataset B')\n",
    "ax.plot(regularizationParam,MSE_C, label = 'Dataset C')\n",
    "plt.xlabel('Alpha (Regularization parameter)')\n",
    "plt.ylabel('Mean-Squared Error')\n",
    "plt.title('lambda (alpha) vs Mean Squared Error for Lasso Regression')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "print('MSE of dataset A = ',MSE_A)\n",
    "print('MSE of dataset B = ',MSE_B)\n",
    "print('MSE of dataset C = ',MSE_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I am recording the values of lambda using dictionaries instead of tables\n",
    "because it is easier to access the items in a dictionary\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "LassodictCoeffsA = dict(zip(regularizationParam,coeffs_dataA))\n",
    "LassodictCoeffsB = dict(zip(regularizationParam,coeffs_dataB))\n",
    "LassodictCoeffsC = dict(zip(regularizationParam,coeffs_dataC))\n",
    "\n",
    "LassodictErrorsA = dict(zip(regularizationParam,MSE_A))\n",
    "LassodictErrorsB = dict(zip(regularizationParam,MSE_B))\n",
    "LassodictErrorsC = dict(zip(regularizationParam,MSE_C))\n",
    "\n",
    "wb_A_Lasso = min(LassodictCoeffsA.items(), key=lambda x: x[1])\n",
    "wb_B_Lasso = min(LassodictCoeffsB.items(), key=lambda x: x[1])\n",
    "wb_C_Lasso = min(LassodictCoeffsC.items(), key=lambda x: x[1])\n",
    "\n",
    "wd_A_Lasso = min(LassodictErrorsA.items(), key=lambda x: x[1])\n",
    "wd_B_Lasso = min(LassodictErrorsB.items(), key=lambda x: x[1])\n",
    "wd_C_Lasso = min(LassodictErrorsC.items(), key=lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from Part 1: Lasso Regression Model###\n",
    "\n",
    "1A) \n",
    "* Number of non-zero coefficients are reported above in print statements for each dataset and for each $\\lambda$ value.\n",
    "\n",
    "1B) The value of Lambda that yields the minimum number of non-zero coefficients:\n",
    "* For dataset A, $\\lambda$ = 0.1, 0.2, 0.3 all yielded zero number of non-zero coefficients (meaning all were zero).\n",
    "* For dataset B, $\\lambda$ = 0.3 yielded 6 non-zero coefficients.\n",
    "* For dataset C, $\\lambda$ = 0.3 yielded 3 non-zero coefficients.\n",
    "\n",
    "1C) \n",
    "* The Mean-Squared Errors are plotted above (second figure). The values are also printed above.\n",
    "\n",
    "1D) \n",
    "* From the final 3 figures, the value of $\\lambda$ with least MSE is:\n",
    "    * For dataset A, $\\lambda$ = 0.05 yielded the $\\mathbf w$ with the least MSE.\n",
    "    * For dataset B, $\\lambda$ = 0.01 yielded the $\\mathbf w$ with the least MSE.\n",
    "    * For dataset C, $\\lambda$ = 0.1 yielded the $\\mathbf w$ with the least MSE.\n",
    "\n",
    "1E)\n",
    "* For dataset A, $\\lambda$ = 0.1, 0.2, 0.3 all already yielded $\\mathbf w$ = 0.\n",
    "    * This shows that for dataset A, increasing $\\lambda$ does indeed reduce the value of $\\mathbf w$ to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now, Ridge regression. I'll use sklearn module `Ridge` (read more [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)). Repeat each of the experiments above using Ridge regression. Use $\\lambda = 1, 50, 100, 200, 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgecoeffs_dataA = []\n",
    "ridgecoeffs_dataB = []\n",
    "ridgecoeffs_dataC = []\n",
    "ridgepredA = []\n",
    "ridgepredB = []\n",
    "ridgepredC = []\n",
    "ridgeRegularizationParam = [1, 50, 100, 200, 1000]\n",
    "#regularizationParam['alpha']\n",
    "\n",
    "\"\"\"\n",
    " Construct Ridge models for each dataset\n",
    " Once the model is trained on training set, use predict method to predict values of y\n",
    " Next, calculate Mean-Squared Errors and also get the count of non-zero coeeficients\n",
    "\"\"\"\n",
    "\n",
    "for value in ridgeRegularizationParam:\n",
    "    \n",
    "    ridgeRegressiondataA = Ridge(alpha=value)\n",
    "    ridgeModeldataA = ridgeRegressiondataA.fit(data_a.train_x, data_a.train_y)\n",
    "    ridgeRegressiondataB = Ridge(alpha = value)\n",
    "    ridgeModeldataB = ridgeRegressiondataB.fit(data_b.train_x, data_b.train_y)\n",
    "    ridgeRegressiondataC = Ridge(alpha = value)\n",
    "    ridgeModeldataC = ridgeRegressiondataC.fit(data_c.train_x, data_c.train_y)\n",
    "    ridgecoeffs_dataA.append(np.count_nonzero(ridgeModeldataA.coef_))\n",
    "    ridgepredA.append(ridgeRegressiondataA.predict(data_a.test_x))\n",
    "    ridgecoeffs_dataB.append(np.count_nonzero(ridgeModeldataB.coef_))\n",
    "    ridgepredB.append(ridgeRegressiondataB.predict(data_b.test_x))\n",
    "    ridgecoeffs_dataC.append(np.count_nonzero(ridgeModeldataC.coef_))\n",
    "    ridgepredC.append(ridgeRegressiondataC.predict(data_c.test_x))\n",
    "    \n",
    "    \"\"\"\n",
    "    The following print statements print out the number of non-zero coefficients for each value of alpha\n",
    "    for each dataset.\n",
    "    They are in reference to first part of question 1A\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Number of non-zero coefficients (datset A) for ALPHA =',value,' is ',np.count_nonzero(ridgeModeldataA.coef_), 'out of ',len(ridgeModeldataA.coef_))\n",
    "    print('Number of non-zero coefficients (dataset B) for ALPHA =',value,' is ',np.count_nonzero(ridgeModeldataB.coef_), 'out of ',len(ridgeModeldataB.coef_))\n",
    "    print('Number of non-zero coefficients (dataset C) for ALPHA =',value,' is ',np.count_nonzero(ridgeModeldataC.coef_), 'out of ',len(ridgeModeldataC.coef_))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the figures ###\n",
    "\n",
    "* First figure plots the number of non-zero coefficients for each dataset for each value of $\\lambda$\n",
    "\n",
    "* Second figure plots the mean squared error for each value of $\\lambda$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(ridgeRegularizationParam, ridgecoeffs_dataA)\n",
    "ax.scatter(ridgeRegularizationParam, ridgecoeffs_dataB)\n",
    "ax.scatter(ridgeRegularizationParam, ridgecoeffs_dataC)\n",
    "\n",
    "ax.plot(ridgeRegularizationParam, ridgecoeffs_dataA,label = 'Dataset A')\n",
    "ax.plot(ridgeRegularizationParam, ridgecoeffs_dataB,label = 'Dataset B')\n",
    "ax.plot(ridgeRegularizationParam, ridgecoeffs_dataC, label = 'Dataset C')\n",
    "plt.title('Alpha vs number of non-zero coefficients for Ridge Regression Model')\n",
    "plt.xlabel('Alpha (Regularization parameter)')\n",
    "plt.ylabel('Number of non-zero coefficients')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "Ridge_MSE_A = []\n",
    "Ridge_MSE_B = []\n",
    "Ridge_MSE_C = []\n",
    " \n",
    "for count in range(len(ridgeRegularizationParam)):\n",
    "    Ridge_MSE_A.append(mean_squared_error(data_a.test_y,ridgepredA[count]))\n",
    "    Ridge_MSE_B.append(mean_squared_error(data_b.test_y,ridgepredB[count]))\n",
    "    Ridge_MSE_C.append(mean_squared_error(data_c.test_y,ridgepredC[count]))\n",
    "    \n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(ridgeRegularizationParam,Ridge_MSE_A)\n",
    "ax.scatter(ridgeRegularizationParam,Ridge_MSE_B)\n",
    "ax.scatter(ridgeRegularizationParam,Ridge_MSE_C)\n",
    "ax.plot(ridgeRegularizationParam,Ridge_MSE_A, label = 'Dataset A')\n",
    "ax.plot(ridgeRegularizationParam,Ridge_MSE_B, label = 'Dataset B')\n",
    "ax.plot(ridgeRegularizationParam,Ridge_MSE_C, label = 'Dataset C')\n",
    "plt.xlabel('Alpha (Regularization parameter)')\n",
    "plt.ylabel('Mean-Squared Error')\n",
    "plt.title('lambda (alpha) vs Mean Squared Error for Ridge Regression Model')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "print('MSE of dataset A = ',Ridge_MSE_A)\n",
    "print('MSE of dataset B = ',Ridge_MSE_B)\n",
    "print('MSE of dataset C = ',Ridge_MSE_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I am recording the values of lambda using dictionaries instead of tables\n",
    "because it is easier to access the items in a dictionary\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "RidgedictCoeffsA = dict(zip(ridgeRegularizationParam,ridgecoeffs_dataA))\n",
    "RidgedictCoeffsB = dict(zip(ridgeRegularizationParam,ridgecoeffs_dataB))\n",
    "RidgedictCoeffsC = dict(zip(ridgeRegularizationParam,ridgecoeffs_dataC))\n",
    "\n",
    "RidgedictErrorsA = dict(zip(ridgeRegularizationParam,Ridge_MSE_A))\n",
    "RidgedictErrorsB = dict(zip(ridgeRegularizationParam,Ridge_MSE_B))\n",
    "RidgedictErrorsC = dict(zip(ridgeRegularizationParam,Ridge_MSE_C))\n",
    "\n",
    "wb_A_Ridge = min(RidgedictCoeffsA.items(), key=lambda x: x[1])\n",
    "wb_B_Ridge = min(RidgedictCoeffsB.items(), key=lambda x: x[1])\n",
    "wb_C_Ridge = min(RidgedictCoeffsC.items(), key=lambda x: x[1])\n",
    "\n",
    "wd_A_Ridge = min(RidgedictErrorsA.items(), key=lambda x: x[1])\n",
    "wd_B_Ridge = min(RidgedictErrorsB.items(), key=lambda x: x[1])\n",
    "wd_C_Ridge = min(RidgedictErrorsC.items(), key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from Part 1: Ridge Regression Model###\n",
    "\n",
    "1A) \n",
    "* Number of non-zero coefficients are reported above in print statements for each dataset and for each $\\lambda$ value.\n",
    "\n",
    "\n",
    "1B) The value of Lambda that yields the minimum number of non-zero coefficients:\n",
    "* For dataset A, no value of $\\lambda$ yielded minimum number of non-zero coefficients (meaning all coefficients were non-zero or no coefficient was zero). \n",
    "    * We can just take the lowest value instead for calcualtion. So take $\\lambda$ = 1\n",
    "*  For dataset A, no value of $\\lambda$ yielded minimum number of non-zero coefficients (meaning all coefficients were non-zero or no coefficient was zero). \n",
    "    * We can just take the lowest value instead for calcualtion. So take $\\lambda$ = 1\n",
    "* For dataset A, no value of $\\lambda$ yielded minimum number of non-zero coefficients (meaning all coefficients were non-zero or no coefficient was zero). \n",
    "    * We can just take the lowest value instead for calcualtion. So take $\\lambda$ = 1\n",
    "    \n",
    "1C) \n",
    "* The Mean-Squared Errors are plotted above (second figure). The values are also printed above.\n",
    "\n",
    "1D) \n",
    "* From the final 3 figures, the value of $\\lambda$ with least MSE is:\n",
    "    * For dataset A, $\\lambda$ = 1000 yielded the $\\mathbf w$ with the least MSE.\n",
    "    * For dataset B, $\\lambda$ = 1 yielded the $\\mathbf w$ with the least MSE.\n",
    "    * For dataset C, $\\lambda$ = 1000 yielded the $\\mathbf w$ with the least MSE.\n",
    "\n",
    "1E)\n",
    "* For dataset A, increasing the value of $\\lambda$ to 100,000 still did not change the number of non-zero coefficients (shown below).\n",
    "    * So even if value of $\\lambda$ is increased beyond normal values, outcome does not change in the context of number of non-zero coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridgequestion1E = Ridge(alpha=100000)\n",
    "ridgetrain1E = Ridgequestion1E.fit(data_a.train_x, data_a.train_y)\n",
    "print('With alpha =100000','number of non-zero coefficients is', np.count_nonzero(ridgetrain1E.coef_),'out of',len(ridgetrain1E.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare the two algorithms on each data set: compare the number of non-zero coordinates of the $\\mathbf w_d$'s, and compare the test error rates of the $\\mathbf w_b$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\tComparison of wb (number of non-zero coeffs)')\n",
    "print('\\t--------------------------------------------\\n')\n",
    "print('\\t Dataset A\\t','Dataset B\\t','Dataset C')\n",
    "print('Lasso\\t',wb_A_Lasso,'\\t',wb_B_Lasso,'\\t',wb_C_Lasso)\n",
    "print('Ridge\\t',wb_A_Ridge,'\\t',wb_B_Ridge,'\\t',wb_C_Ridge)\n",
    "print('\\n\\n')\n",
    "print('\\tComparison of wd (test error rates or MSE)')\n",
    "print('\\t--------------------------------------------\\n')\n",
    "print('\\t Dataset A\\t','Dataset B\\t','Dataset C')\n",
    "print('Lasso\\t',wd_A_Lasso,'\\t',wd_B_Lasso,'\\t',wd_C_Lasso)\n",
    "print('Ridge\\t',wd_A_Ridge,'\\t',wd_B_Ridge,'\\t',wd_C_Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two table comparisons, I can make the following comments:\n",
    "\n",
    "* In regards to wb i.e. in the context of least number of non-zero coefficients, Lasso has the least in all three datasets. \n",
    "    * This is particularly because Ridge smooths more with increase in alpha. As alpha increases, the fit becomes more and more strict.\n",
    "    * Only if the alpha value is very small, the coefficients vary a lot. As alpha increases, the coefficients become more defined and the fit of the curve becomes strict and tight.\n",
    "\n",
    "* In the context of wd i.e. the least error rate, Ridge and Lasso exhibit very similar error rates and are nearly equal.\n",
    "    * Dataset B gets the least error rate in the order of 10^-2. Error rates in dataset A and B are nearly equal for both regression models.\n",
    "    * It must be noted that while both dataset A and C experienced min. error rate at $\\lambda$ = 1000, dataset B experiences the min. error rate at $\\lambda$ = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing Data and Decision Boundaries for different kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Training SVM with Linear Kernel (Dataset 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_data import plot_data\n",
    "\n",
    "\n",
    "# Load from data1\n",
    "mat_data = sio.loadmat('./data/data1.mat')\n",
    "X = mat_data['X']\n",
    "y = mat_data['y'].ravel()\n",
    "\n",
    "# Plot training data\n",
    "plt.figure()\n",
    "plot_data(X, y)\n",
    "plt.xlim([0, 4.5])\n",
    "plt.ylim([1.5, 5])\n",
    "plt.title(\"Dataset 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** In `data1.mat`, most of the positive and negative examples can be separated by a single line. Train SVM with linear kernel with C = 1 and plot the decision boundary using `visualize_boundary_linear(X, y, clf)`. `clf` is the SVM classifier. For the classifier, I will use the scikit-learn implementation (Feel free to experiment with different values of C and see what effect it has on the decision boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "from visualize_boundary_linear import visualize_boundary_linear\n",
    "\n",
    "\n",
    "# 1. Create a linear SVM classifier\n",
    "clf = svm.LinearSVC(C=1)\n",
    "\n",
    "# 2. Fit the model according to the given training data.\n",
    "clf.fit(X,y)\n",
    "\n",
    "# 3. Print the mean accuracy on the given train data and labels using the score function in scikit-learn\n",
    "print('Score for dataset 1 with linear boundary is',clf.score(X,y),'or',clf.score(X,y)*100,'%')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "# Uncomment the below line after you build your classifier\n",
    "visualize_boundary_linear(X, y, clf)\n",
    "plt.xlim([0, 4.5])\n",
    "plt.ylim([1.5, 5])\n",
    "plt.title(\"Dataset 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Training SVM with RBF Kernel (Dataset 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from data2\n",
    "mat_data = sio.loadmat('./data/data2.mat')\n",
    "X = mat_data['X']\n",
    "y = mat_data['y'].ravel()\n",
    "\n",
    "# Plot training data\n",
    "plt.figure()\n",
    "plot_data(X, y)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0.4, 1])\n",
    "plt.title(\"Dataset 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B** In `data2.mat`, the positive and negative data points are not linearly separable. For this dataset, construct an SVM classifier with a Gaussian kernel to learn a non-linear decision boundary. I use the scikit-learn implementation for the same. To plot the decision boundary use `visualize_boundary(X, y, clf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_boundary import visualize_boundary\n",
    "\n",
    "# SVM Parameters to be used\n",
    "C = 100\n",
    "gamma = 10\n",
    "\n",
    "### START CODE HERE ### (approx. 3 lines)\n",
    "# 1. Create a SVM classifier with kernel='rbf'\n",
    "clf = svm.SVC(kernel = 'rbf',gamma=gamma, C=C)\n",
    "\n",
    "# 2. Fit the model according to the given training data.\n",
    "clf.fit(X,y)\n",
    "\n",
    "# 3. Print the mean accuracy on the given train data and labels using the score function in scikit-learn\n",
    "print('Score for dataset 2 with non-linear boundary is ', clf.score(X,y),'or', clf.score(X,y)*100,'%')\n",
    "### END CODE HERE ### \n",
    "\n",
    "plt.figure()\n",
    "# Uncomment the below line after you build your classifier\n",
    "visualize_boundary(X, y, clf)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0.4, 1])\n",
    "plt.title(\"Dataset 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kernel Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will be implementing the Kernel Perceptron using different kernel functions. I will then use the training and test data provided below to train and test the implementation. Finally i will have to report the error rate as a percentage for each kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Implement the kernel functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Linear kernel = transpose(x)*y\n",
    "\n",
    "Polynomial kernel = (1 + trasnpose(x)*y)^p\n",
    "\n",
    "Gaussian kernel = exp((-(x-y)^2)/2*(sigma^2))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def linear_kernel(x1, x2):\n",
    "    #  TODO\n",
    "    #pass\n",
    "    return np.dot(x1,x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p = 3):\n",
    "    #  TODO\n",
    "    #  p = degree of the polynomial\n",
    "    #pass\n",
    "    return (1 + np.dot(x,y))**p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma = 0.5):\n",
    "    #     TODO:\n",
    "    #pass\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestKernels(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.x1 = np.array([1, 2, 1])\n",
    "        self.x2 = np.array([0, 4, -1])\n",
    "        \n",
    "    def test0(self):\n",
    "        \"\"\"\n",
    "        Test the linear kernel\n",
    "        \"\"\"\n",
    "        self.assertEqual(linear_kernel(self.x1, self.x2), 7)\n",
    "    \n",
    "    def test_polynomial_kernel(self):\n",
    "        \"\"\"\n",
    "        Test the polynomial kernel\n",
    "        \"\"\"\n",
    "        self.assertEqual(polynomial_kernel(self.x1, self.x2), 512)\n",
    "        \n",
    "    def test_gaussian_kernel(self):\n",
    "        \"\"\"\n",
    "        Test the gaussian kernel\n",
    "        \"\"\"\n",
    "        self.assertAlmostEqual(gaussian_kernel(self.x1, self.x2) * 10 ** 8, 1.52299, 4)\n",
    "    \n",
    "\n",
    "tests = TestKernels()\n",
    "tests_to_run = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(tests_to_run)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    def __init__(self, kernel = linear_kernel, Niter = 1):\n",
    "        self.kernel = kernel\n",
    "        self.Niter = Niter\n",
    "        self.support_vector_x = None\n",
    "        self.support_vector_y = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # TODO: \n",
    "        #pass\n",
    "        \n",
    "        training_examples, training_features = X.shape\n",
    "        #print('training examples=',training_examples,'also training features=',training_features)\n",
    "        #print('type of kernel',self.kernel)\n",
    "        \n",
    "        # initialize alpha to zero\n",
    "        self.alpha = np.zeros(training_examples, dtype=np.float64)\n",
    "        \n",
    "        # Took a hint from the Piazza discussion and came up with this\n",
    "        K = np.zeros((training_examples, training_examples))\n",
    "        #print('Size of k is',K.shape)\n",
    "        for i in range(training_examples):\n",
    "            for j in range(training_examples):\n",
    "                #print('i=',i,'j=',j)\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "                #print('Kij=',K[i,j])\n",
    "        \n",
    "        # now update alpha\n",
    "        # if the y value matches, do nothing\n",
    "        # if it doesn't match, increment by 1\n",
    "        \n",
    "        # again, took a hint from Piazza discussions\n",
    "        for iterations in range(self.Niter):\n",
    "            for index in range(training_examples):\n",
    "                if np.sign(np.sum(K[:,index] * self.alpha * y)) != y[index]:\n",
    "                    self.alpha[index] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # set the support vector to take all non-zero values of alpha\n",
    "        \n",
    "        supportVector = self.alpha > 0.00001 # or just take non-zero \n",
    "        #index = np.arrange(len(self.alpha))[supportVector]\n",
    "        self.alpha = self.alpha[supportVector]\n",
    "        self.support_vector_x = X[supportVector]\n",
    "        self.support_vector_y = y[supportVector]\n",
    "       \n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "        # TODO:\n",
    "        predicted_y = np.zeros(len(X))\n",
    "        #print('length of X',len(X))\n",
    "        #print('zip',zip(self.alpha,self.support_vector_y,self.support_vector_x))\n",
    "        for i in range(len(X)):\n",
    "            \n",
    "            # predict function is given by:\n",
    "            # y = sign(sum(alpha(i)*y(i)*kernel(X(i),x)))\n",
    "            \n",
    "            for a,b,c in zip(self.alpha,self.support_vector_y,self.support_vector_x):\n",
    "                predicted_y[i]+=a*b*self.kernel(X[i],c)\n",
    "          \n",
    "        return np.sign(predicted_y)\n",
    "       \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Solving a new classification problem using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the IRIS Dataset for this classification task. We have created a binary classification problem to determine whether a given flower is a setosa or not. To create this, we pre-processed the labels to create a label vector where setosa’s label is unchanged (i.e. its label is 1), but both versicolor and virginica are now labeled as -1. The data contains two out of the four attributes, petal width and petal length.We are going to use this dataset to test our Kernel Perceptron\n",
    "<br>\n",
    "\n",
    "<img src=\"wide_iris.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        ff = lambda x,y : loadmat(x)[y]\n",
    "        \n",
    "        self.X_train = ff(\"data/iris_3/train_data.mat\", \"train_data\")\n",
    "        self.y_train = ff(\"data/iris_3/train_labels.mat\", \"train_labels\").flatten()\n",
    "        \n",
    "        self.X_test = ff(\"data/iris_3/test_data.mat\", \"test_data\")\n",
    "        self.y_test = ff(\"data/iris_3/test_labels.mat\", \"test_labels\").flatten()\n",
    "        \n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Kernel ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptLinear = KernelPerceptron(Niter=20, kernel=linear_kernel)\n",
    "perceptLinear.fit(data.X_train,data.y_train)\n",
    "y_predicted = perceptLinear.predict(data.X_test)\n",
    "\n",
    "accuracy_count = 0\n",
    "\n",
    "for i in range(len(data.y_test)):\n",
    "    if y_predicted[i] == data.y_test[i]:\n",
    "        accuracy_count +=1\n",
    "    \n",
    "print('accuracy score =',accuracy_count*100/len(data.y_test),'%','and error score is',(len(data.y_test)-accuracy_count)*100/len(data.y_test),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptPolynomial = KernelPerceptron(Niter=20, kernel=polynomial_kernel)\n",
    "perceptPolynomial.fit(data.X_train,data.y_train)\n",
    "y_predicted = perceptPolynomial.predict(data.X_test)\n",
    "\n",
    "accuracy_count = 0\n",
    "\n",
    "for i in range(len(data.y_test)):\n",
    "    if y_predicted[i] == data.y_test[i]:\n",
    "        accuracy_count +=1\n",
    "\n",
    "print('accuracy score =',accuracy_count*100/len(data.y_test),'%','and error score is',(len(data.y_test)-accuracy_count)*100/len(data.y_test),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Kernel ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptGaussian = KernelPerceptron(Niter=20, kernel=gaussian_kernel)\n",
    "perceptGaussian.fit(data.X_train,data.y_train)\n",
    "y_predicted = perceptGaussian.predict(data.X_test)\n",
    "\n",
    "accuracy_count = 0\n",
    "\n",
    "for i in range(len(data.y_test)):\n",
    "    if y_predicted[i] == data.y_test[i]:\n",
    "        accuracy_count +=1\n",
    "\n",
    "print('accuracy score =',accuracy_count*100/len(data.y_test),'%','and error score is',(len(data.y_test)-accuracy_count)*100/len(data.y_test),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C:** Report the test error as a percentage for each kernel function for `Niter = 20`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error scores ###\n",
    "\n",
    "* The perceptron was run with three different kernels (linear, polynomial and Gaussian) for 20 iterations `Niter=20`\n",
    "\n",
    "* The test errors match the expected output:\n",
    "    1. Linear kernel has an accuracy of 60% and error score of 40%\n",
    "\n",
    "    2. Polynomial kernel has an accuracy of 93.33% and error score of 6.67%\n",
    "\n",
    "    3. Gaussian kernel has an accuracy of 93.33% and error score of 6.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of document ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
